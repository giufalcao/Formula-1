{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Start Spark Session and Include additional configurations and common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/configurations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/common_functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/04 18:35:07 WARN Utils: Your hostname, falcao-sys resolves to a loopback address: 127.0.1.1; using 192.168.11.185 instead (on interface wlx7898e8c12476)\n",
      "24/01/04 18:35:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/04 18:35:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, FloatType\n",
    "\n",
    "appName = \"Formula 1 Database\"\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = create_spark_context(appName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2 - Create F1 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the database if it doesn't exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS f1_raw\")\n",
    "\n",
    "# Switch to the newly created database\n",
    "spark.sql(\"USE f1_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Create and Save Circuits table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_schema = StructType(fields=[\n",
    "    StructField(\"circuitId\", IntegerType(), False),\n",
    "    StructField(\"circuitRef\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lng\", DoubleType(), True),\n",
    "    StructField(\"alt\", IntegerType(), True),\n",
    "    StructField(\"url\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_df = spark.read.option(\"header\", True).schema(circuits_schema).csv(f\"{data_folder_path}/circuits.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "circuits_df.write.saveAsTable(\"circuits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Create Races table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_schema = StructType(fields=[\n",
    "    StructField(\"raceId\", IntegerType(), False),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"round\", IntegerType(), True),\n",
    "    StructField(\"circuitId\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"date\", DateType(), False),\n",
    "    StructField(\"time\", StringType(), False),\n",
    "    StructField(\"url\", StringType(), True)\n",
    "])\n",
    "\n",
    "races_df = spark.read.option(\"header\", True).schema(races_schema).csv(\"../data/races.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_df.write.saveAsTable(\"races\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Create Constructors table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructors_schema = StructType(fields=[\n",
    "    StructField(\"constructorId\", IntegerType(), False),\n",
    "    StructField(\"constructorRef\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True)\n",
    "])\n",
    "\n",
    "constructor_df = spark.read.schema(constructors_schema).json(\"../data/constructors.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructor_df.write.saveAsTable(\"constructors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Create Drivers table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_schema = StructType(fields=[\n",
    "    StructField(\"forename\", StringType(), True),\n",
    "    StructField(\"surname\", StringType(), True)\n",
    "])\n",
    "\n",
    "drives_schema = StructType(fields=[\n",
    "    StructField(\"driverId\", IntegerType(), False),\n",
    "    StructField(\"driverRef\", StringType(), True),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"code\", StringType(), True),\n",
    "    StructField(\"name\", name_schema),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True)\n",
    "])\n",
    "\n",
    "drives_df = spark.read.schema(drives_schema).json(\"../data/drivers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drives_df.write.saveAsTable(\"drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Create Results table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_schema = StructType(fields=[\n",
    "    StructField(\"resultId\", IntegerType(), False),\n",
    "    StructField(\"raceId\", IntegerType(), False),\n",
    "    StructField(\"driverId\", IntegerType(), False),\n",
    "    StructField(\"constructorId\", IntegerType(), False),\n",
    "    StructField(\"number\", IntegerType(), True),\n",
    "    StructField(\"grid\", IntegerType(), False),\n",
    "    StructField(\"position\", IntegerType(), True),\n",
    "    StructField(\"positionText\", StringType(), False),\n",
    "    StructField(\"positionOrder\", IntegerType(), False),\n",
    "    StructField(\"points\", FloatType(), False),\n",
    "    StructField(\"laps\", IntegerType(), False),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"milliseconds\", IntegerType(), True),\n",
    "    StructField(\"fastestLap\", IntegerType(), True),\n",
    "    StructField(\"rank\", IntegerType(), True),\n",
    "    StructField(\"fastestLapTime\", StringType(), True),\n",
    "    StructField(\"fastestLapSpeed\", FloatType(), True),\n",
    "    StructField(\"statusId\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "results_df = spark.read.schema(results_schema).json(\"../data/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_df.write.saveAsTable(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Create Pit Stops table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_stops_schema = StructType(fields=[\n",
    "    StructField(\"raceId\", IntegerType(), False),\n",
    "    StructField(\"driverId\", IntegerType(), False),\n",
    "    StructField(\"stop\", StringType(), False),\n",
    "    StructField(\"lap\", IntegerType(), False),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"duration\", StringType(), False),\n",
    "    StructField(\"milliseconds\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "pit_stops_df = spark.read.schema(pit_stops_schema).option(\"multiLine\", True).json(\"../data/pit_stops.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pit_stops_df.write.saveAsTable(\"pit_stops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Create Lap Times table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lap_times_schema = StructType(fields=[\n",
    "    StructField(\"raceId\", IntegerType(), False),\n",
    "    StructField(\"driverId\", IntegerType(), False),\n",
    "    StructField(\"position\", IntegerType(), False),\n",
    "    StructField(\"lap\", IntegerType(), False),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"milliseconds\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "lap_times_df = spark.read.schema(lap_times_schema).csv(\"../data/lap_times/lap_times_split*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lap_times_df.write.saveAsTable(\"lap_times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 - Create Qualifying table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying_schema = StructType(fields=[\n",
    "    StructField(\"qualifyId\", IntegerType(), False),\n",
    "    StructField(\"raceId\", IntegerType(), False),\n",
    "    StructField(\"driverId\", IntegerType(), False),\n",
    "    StructField(\"constructorId\", IntegerType(), False),\n",
    "    StructField(\"number\", IntegerType(), False),\n",
    "    StructField(\"position\", IntegerType(), False),\n",
    "    StructField(\"q1\", StringType(), True),\n",
    "    StructField(\"q2\", StringType(), False),\n",
    "    StructField(\"q3\", StringType(), True)\n",
    "])\n",
    "\n",
    "qualifying_df = spark.read.schema(qualifying_schema).option(\"multiLine\", True).json(\"../data/qualifying/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualifying_df.write.saveAsTable(\"qualifying\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
