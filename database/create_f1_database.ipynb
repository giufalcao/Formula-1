{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Start Spark Session and Include additional configurations and common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/configurations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../includes/common_functions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:26 WARN Utils: Your hostname, falcao-sys resolves to a loopback address: 127.0.1.1; using 192.168.11.185 instead (on interface wlx7898e8c12476)\n",
      "24/01/02 16:45:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/02 16:45:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"F1Database\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hive/\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2 - Create F1 Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:31 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/01/02 16:45:31 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/01/02 16:45:34 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/01/02 16:45:34 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore giu@127.0.1.1\n",
      "24/01/02 16:45:34 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the database if it doesn't exist\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS f1_raw\")\n",
    "\n",
    "# Switch to the newly created database\n",
    "spark.sql(\"USE f1_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Create Circuits table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:36 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`f1_raw`.`circuits` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/01/02 16:45:36 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/01/02 16:45:36 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/01/02 16:45:36 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/01/02 16:45:36 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+------------+---------+--------+---------+---+--------------------+\n",
      "|circuitId|    circuitRef|                name|    location|  country|     lat|      lng|alt|                 url|\n",
      "+---------+--------------+--------------------+------------+---------+--------+---------+---+--------------------+\n",
      "|        1|   albert_park|Albert Park Grand...|   Melbourne|Australia|-37.8497|  144.968| 10|http://en.wikiped...|\n",
      "|        2|        sepang|Sepang Internatio...|Kuala Lumpur| Malaysia| 2.76083|  101.738| 18|http://en.wikiped...|\n",
      "|        3|       bahrain|Bahrain Internati...|      Sakhir|  Bahrain| 26.0325|  50.5106|  7|http://en.wikiped...|\n",
      "|        4|     catalunya|Circuit de Barcel...|    Montmeló|    Spain|   41.57|  2.26111|109|http://en.wikiped...|\n",
      "|        5|      istanbul|       Istanbul Park|    Istanbul|   Turkey| 40.9517|   29.405|130|http://en.wikiped...|\n",
      "|        6|        monaco|   Circuit de Monaco| Monte-Carlo|   Monaco| 43.7347|  7.42056|  7|http://en.wikiped...|\n",
      "|        7|    villeneuve|Circuit Gilles Vi...|    Montreal|   Canada|    45.5| -73.5228| 13|http://en.wikiped...|\n",
      "|        8|   magny_cours|Circuit de Nevers...| Magny Cours|   France| 46.8642|  3.16361|228|http://en.wikiped...|\n",
      "|        9|   silverstone| Silverstone Circuit| Silverstone|       UK| 52.0786| -1.01694|153|http://en.wikiped...|\n",
      "|       10|hockenheimring|      Hockenheimring|  Hockenheim|  Germany| 49.3278|  8.56583|103|http://en.wikiped...|\n",
      "|       11|   hungaroring|         Hungaroring|    Budapest|  Hungary| 47.5789|  19.2486|264|http://en.wikiped...|\n",
      "|       12|      valencia|Valencia Street C...|    Valencia|    Spain| 39.4589|-0.331667|  4|http://en.wikiped...|\n",
      "|       13|           spa|Circuit de Spa-Fr...|         Spa|  Belgium| 50.4372|  5.97139|401|http://en.wikiped...|\n",
      "|       14|         monza|Autodromo Naziona...|       Monza|    Italy| 45.6156|  9.28111|162|http://en.wikiped...|\n",
      "|       15|    marina_bay|Marina Bay Street...|  Marina Bay|Singapore|  1.2914|  103.864| 18|http://en.wikiped...|\n",
      "|       16|          fuji|       Fuji Speedway|       Oyama|    Japan| 35.3717|  138.927|583|http://en.wikiped...|\n",
      "|       17|      shanghai|Shanghai Internat...|    Shanghai|    China| 31.3389|   121.22|  5|http://en.wikiped...|\n",
      "|       18|    interlagos|Autódromo José Ca...|   São Paulo|   Brazil|-23.7036| -46.6997|785|http://en.wikiped...|\n",
      "|       19|  indianapolis|Indianapolis Moto...|Indianapolis|      USA|  39.795| -86.2347|223|http://en.wikiped...|\n",
      "|       20|   nurburgring|         Nürburgring|     Nürburg|  Germany| 50.3356|   6.9475|578|http://en.wikiped...|\n",
      "+---------+--------------+--------------------+------------+---------+--------+---------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.circuits\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.circuits (\n",
    "    circuitId INT,\n",
    "    circuitRef STRING,\n",
    "    name STRING,\n",
    "    location STRING,\n",
    "    country STRING,\n",
    "    lat DOUBLE,\n",
    "    lng DOUBLE,\n",
    "    alt INT,\n",
    "    url STRING\n",
    "  )\n",
    "  USING csv\n",
    "  OPTIONS (path '../../data/circuits.csv', header 'true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Create Races table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:39 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`f1_raw`.`races` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.races\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.races(\n",
    "    raceId INT,\n",
    "    year INT,\n",
    "    round INT,\n",
    "    circuitId INT,\n",
    "    name STRING,\n",
    "    date DATE,\n",
    "    time STRING,\n",
    "    url STRING\n",
    "  )\n",
    "  USING csv\n",
    "  OPTIONS (path \"../../data/races.csv\", header true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Create Constructors table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:39 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider json. Persisting data source table `spark_catalog`.`f1_raw`.`constructors` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.constructors\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.constructors(\n",
    "    constructorId INT,\n",
    "    constructorRef STRING,\n",
    "    name STRING,\n",
    "    nationality STRING,\n",
    "    url STRING\n",
    "  )\n",
    "  USING json\n",
    "  OPTIONS(path \"../../data/constructors.json\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - Create Drivers table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:39 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider json. Persisting data source table `spark_catalog`.`f1_raw`.`drivers` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.drivers\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.drivers(\n",
    "    driverId INT,\n",
    "    driverRef STRING,\n",
    "    number INT,\n",
    "    code STRING,\n",
    "    name STRUCT<forename: STRING, surname: STRING>,\n",
    "    dob DATE,\n",
    "    nationality STRING,\n",
    "    url STRING\n",
    "  )\n",
    "  USING json\n",
    "  OPTIONS (path \"../../data/drivers.json\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - Create Results table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:39 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider json. Persisting data source table `spark_catalog`.`f1_raw`.`results` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.results\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.results(\n",
    "    resultId INT,\n",
    "    raceId INT,\n",
    "    driverId INT,\n",
    "    constructorId INT,\n",
    "    number INT,grid INT,\n",
    "    position INT,\n",
    "    positionText STRING,\n",
    "    positionOrder INT,\n",
    "    points INT,\n",
    "    laps INT,\n",
    "    time STRING,\n",
    "    milliseconds INT,\n",
    "    fastestLap INT,\n",
    "    rank INT,\n",
    "    fastestLapTime STRING,\n",
    "    fastestLapSpeed FLOAT,\n",
    "    statusId STRING\n",
    "  )\n",
    "  USING json\n",
    "  OPTIONS(path \"../../data/results.json\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - Create Pit Stops table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:39 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider json. Persisting data source table `spark_catalog`.`f1_raw`.`pit_stops` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.pit_stops\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.pit_stops(\n",
    "    driverId INT,\n",
    "    duration STRING,\n",
    "    lap INT,\n",
    "    milliseconds INT,\n",
    "    raceId INT,\n",
    "    stop INT,\n",
    "    time STRING\n",
    "  )\n",
    "  USING json\n",
    "  OPTIONS(path \"../../data/pit_stops.json\", multiLine true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Create Lap Times table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:45:39 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `spark_catalog`.`f1_raw`.`lap_times` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.lap_times\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.lap_times(\n",
    "    raceId INT,\n",
    "    driverId INT,\n",
    "    lap INT,\n",
    "    position INT,\n",
    "    time STRING,\n",
    "    milliseconds INT\n",
    "  )\n",
    "  USING csv\n",
    "  OPTIONS (path \"../../data/lap_times\")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - Create Qualifying table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 16:46:04 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider json. Persisting data source table `spark_catalog`.`f1_raw`.`qualifying` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS f1_raw.qualifying\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS f1_raw.qualifying(\n",
    "    constructorId INT,\n",
    "    driverId INT,\n",
    "    number INT,\n",
    "    position INT,\n",
    "    q1 STRING,\n",
    "    q2 STRING,\n",
    "    q3 STRING,\n",
    "    qualifyId INT,\n",
    "    raceId INT\n",
    "  )\n",
    "  USING json\n",
    "  OPTIONS (path \"../../data/qualifying\", multiLine true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------+--------+--------+--------+--------+---------+------+\n",
      "|constructorId|driverId|number|position|      q1|      q2|      q3|qualifyId|raceId|\n",
      "+-------------+--------+------+--------+--------+--------+--------+---------+------+\n",
      "|            1|       1|    22|       1|1:26.572|1:25.187|1:26.714|        1|    18|\n",
      "|            2|       9|     4|       2|1:26.103|1:25.315|1:26.869|        2|    18|\n",
      "|            1|       5|    23|       3|1:25.664|1:25.452|1:27.079|        3|    18|\n",
      "|            6|      13|     2|       4|1:25.994|1:25.691|1:27.178|        4|    18|\n",
      "|            2|       2|     3|       5|1:25.960|1:25.518|1:27.236|        5|    18|\n",
      "|            7|      15|    11|       6|1:26.427|1:26.101|1:28.527|        6|    18|\n",
      "|            3|       3|     7|       7|1:26.295|1:26.059|1:28.687|        7|    18|\n",
      "|            9|      14|     9|       8|1:26.381|1:26.063|1:29.041|        8|    18|\n",
      "|            7|      10|    12|       9|1:26.919|1:26.164|1:29.593|        9|    18|\n",
      "|            5|      20|    15|      10|1:26.702|1:25.842|      \\N|       10|    18|\n",
      "|           11|      22|    17|      11|1:26.369|1:26.173|      \\N|       11|    18|\n",
      "|            4|       4|     5|      12|1:26.907|1:26.188|      \\N|       12|    18|\n",
      "|           11|      18|    16|      13|1:26.712|1:26.259|      \\N|       13|    18|\n",
      "|            3|       6|     8|      14|1:26.891|1:26.413|      \\N|       14|    18|\n",
      "|            9|      17|    10|      15|1:26.914|      \\N|      \\N|       15|    18|\n",
      "|            6|       8|     1|      16|1:26.140|      \\N|      \\N|       16|    18|\n",
      "|           10|      21|    21|      17|1:27.207|      \\N|      \\N|       17|    18|\n",
      "|            5|       7|    14|      18|1:27.446|      \\N|      \\N|       18|    18|\n",
      "|           10|      16|    20|      19|1:27.859|      \\N|      \\N|       19|    18|\n",
      "|            8|      11|    18|      20|1:28.208|      \\N|      \\N|       20|    18|\n",
      "+-------------+--------+------+--------+--------+--------+--------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
